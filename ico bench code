from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from bs4 import BeautifulSoup
import re
import pandas as pd
import os
import time
from selenium.webdriver.support.ui import Select

working_directory='C:/ico bench/collection'
os.chdir(working_directory)
os.makedirs('milestones')
no_of_pages=10 #mention the number of pages
common_href='https://icobench.com'
pageturn_href='https://icobench.com'
crypto_name=[]
crypto_description_long=[]
crypto_rating_benchy=[]
crypto_rating_team=[]
crypto_rating_vision=[]
crypto_rating_product=[]
financial_data_token=[]
financial_data_Type=[]
financial_data_PreICOPrice=[]
financial_data_Price=[]
financial_data_Bounty=[]
financial_data_MVP=[]
financial_data_Pltform=[]
financial_data_Accepting=[]
financial_data_Minimum_investment=[]
financial_data_Softcap=[]
financial_data_Hardcap=[]
financial_data_CountryWhitelist=[]
financial_data_whitelistorKYC=[]
crypto_description_short=[]
crypto_info_name=[]
crypto_info_platform=[]
crypto_info_type=[]
crypto_info_preICOprice=[]
crypto_info_priceInICO=[]
crypto_info_tokenForSale=[]
crypto_info_MinInvestments=[]
crypto_info_Accepting=[]
crypto_info_DistributedInICO=[]
crypto_info_SoftCap=[]
crypto_info_HardCap=[]
crypto_info_Raised=[]
whitepaper_status=[]
token_info=[]
token_info_values=[]
name=[]


Coins=pd.DataFrame()
Financial_table=pd.DataFrame()    
driver = webdriver.Chrome('C:/chromedriver.exe')
driver_inner = webdriver.Chrome('C:/chromedriver.exe')
driver.get('https://icobench.com/icos')
#driver = webdriver.Chrome('C:/unzipped/chromedriver_win32/chromedriver.exe')
main_table=pd.DataFrame()


# #this part of code will collect fields for uniformity
# driver_fields = webdriver.Chrome('C:/chromedriver.exe')
# #driver = webdriver.Chrome('C:/unzipped/chromedriver_win32/chromedriver.exe')
# driver_fields.get('https://icobench.com/ico/p2p-solutions-foundation')
# financial_data_div=page_soup.findAll("div",{'class':'financial_data'})[0]
# k1=financial_data_div.findAll('div',{'class':'data_row'})[0].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k2=financial_data_div.findAll('div',{'class':'data_row'})[1].findAll('div',{'class':'col_2'})[0].text
# k3=financial_data_div.findAll('div',{'class':'data_row'})[2].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k4=financial_data_div.findAll('div',{'class':'data_row'})[3].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k4=financial_data_div.findAll('div',{'class':'data_row'})[4].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k5=financial_data_div.findAll('div',{'class':'data_row'})[5].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k6=financial_data_div.findAll('div',{'class':'data_row'})[6].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k7=financial_data_div.findAll('div',{'class':'data_row'})[7].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k8=financial_data_div.findAll('div',{'class':'data_row'})[8].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k9=financial_data_div.findAll('div',{'class':'data_row'})[9].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k10=financial_data_div.findAll('div',{'class':'data_row'})[10].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k11=financial_data_div.findAll('div',{'class':'data_row'})[11].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# k12=financial_data_div.findAll('div',{'class':'data_row'})[12].findAll('div',{'class':'col_2'})[0].text.split('\n')[1].split('\t')[-1]
# fields=[k1,k2,k3,k4,k5,k6,k7,k8,k9,k10,k11,k12]



for pages in range(no_of_pages):
    list_of_href=[]
    page_soup=BeautifulSoup(driver.page_source,'lxml') #converting webpage into soup object
    find_div_section = page_soup.findAll("div",{'class':'ico_list'}) #finding list of div section conatining specified table
    div_section = find_div_section[0]
    table=div_section.table #getting table from div element
    page_table=pd.read_html(str(table),header=0)[0]
    main_table=pd.concat([main_table,page_table],ignore_index=True)
    href_iteration = div_section.findAll("a",{'class':'name notranslate'})
    list_of_href=[]
    for items in href_iteration:
        list_of_href.append(items.get('href'))
    next_button=page_soup.findAll("a",{'class':'next'})[0].get('href')
    driver.get(pageturn_href+next_button)
    for href in list_of_href:
        
        try:
            driver_inner.get(common_href+href+'/milestones')
            milestones=pd.DataFrame()
            page_soup=BeautifulSoup(driver_inner.page_source,'lxml') #converting webpage into soup object
            find_div_section = page_soup.findAll("div",{'class':'tab_content'}) #finding list of div section conatining specified table
            div_section = find_div_section[0]
            conditions_iteration = div_section.findAll("div",{'class':'condition'})
            event_iteration = div_section.findAll("p",{'class':''})
            milestones = pd.DataFrame()
            conditions=[]
            event=[]
            for iterations in conditions_iteration:
                conditions.append(iterations.text)
            for iterations in event_iteration:
                event.append(iterations.text)
            milestones['Timeline']=pd.Series(conditions)
            milestones['Event']=pd.Series(event)
            
            os.chdir(working_directory+'/milestones')
            milestones.to_csv('{}_milestones.csv'.format(href.split('/')[2]))
            os.chdir(working_directory)
            time.sleep(1)
        except:
            time.sleep(1)
 
        url = common_href+href
        driver_inner.get(url)
        page_soup=BeautifulSoup(driver_inner.page_source,'lxml') #converting webpage into soup object
        try:
            crypto_name.append(page_soup.findAll("div",{'class':'name'})[0].h1.text)
        except:
            crypto_name.append('')  
        try:
            crypto_description_long.append(page_soup.findAll("div",{'class':'ico_information'})[0].p.text)
        except:
            crypto_description_long.append('')
        try:
            crypto_rating_benchy.append(page_soup.findAll("div",{'class':'wrapper'})[0].text.split('\n')[2].split(' ')[-1])
        except:
            crypto_rating_benchy.append('')
        try:
            crypto_rating_div=page_soup.findAll("div",{'class':'wrapper'})[1]
        except:
            crypto_rating_div=''
        
        
        try:
            crypto_rating_team.append(crypto_rating_div.findAll("div",{'class':'col_4 col_3'})[0].text.split('\n')[1].split(' ')[-1])
        except:
            crypto_rating_team.append('')
        
        try:
            crypto_rating_vision.append(crypto_rating_div.findAll("div",{'class':'col_4 col_3'})[1].text.split('\n')[1].split(' ')[-1])
        except:
            crypto_rating_vision.append('')
        
        try:
            crypto_rating_product.append(crypto_rating_div.findAll("div",{'class':'col_4 col_3'})[2].text.split('\n')[1].split(' ')[-1])
        except:
            crypto_rating_product.append('')
        
        
        
        
       
        financial_url=common_href+href+'/financial'
        driver_inner.get(financial_url)
        page_soup=BeautifulSoup(driver_inner.page_source,'lxml')

        
        crypto_info_div=page_soup.findAll("div",{'class':'tab_content'})[0]
        box_right_div=crypto_info_div.findAll("div",{'class':'box_right'})[0]
        rows_div_right=box_right_div.findAll("div",{'class':'row'})
        
        for x in range(len(rows_div_right)):
            token_info.append(rows_div_right[x].findAll("div")[0].text)
            token_info_values.append(rows_div_right[x].findAll("div")[-1].text)
        box_right_div=crypto_info_div.findAll("div",{'class':'box_left'})[0]
        rows_div_left=box_right_div.findAll("div",{'class':'row'})
        # investment_info=[]
        # investment_info_values=[]
        for x in range(len(rows_div_left)):
            token_info.append(rows_div_left[x].findAll("div")[0].text)
            token_info_values.append(rows_div_left[x].findAll("div")[-1].text)
        
        for x in range(len(rows_div_right+rows_div_left)):
            name.append(crypto_name[-1])
        Financial_table=pd.DataFrame()
        Financial_table['Token_info']=token_info
        Financial_table['Token_table']=token_info_values
        Financial_table['Token_name']=name
        Financial_table.to_csv('Financial_data.csv')        
        continue
            

Coins['crypto_name']=pd.Series(crypto_name)
Coins['crypto_description_long']=pd.Series(crypto_description_long)
Coins['crypto_rating_benchy']=pd.Series(crypto_rating_benchy)
Coins['crypto_rating_team']=pd.Series(crypto_rating_team)
Coins['crypto_rating_vision']=pd.Series(crypto_rating_vision)
Coins['crypto_rating_product']=pd.Series(crypto_rating_product)
Coins.to_csv('Coin.csv') 


